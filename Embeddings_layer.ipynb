{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kltg0p5EUhe1"
      },
      "source": [
        "**NB: in this notebook no training was done, therefore all the values resulting in the embedding vecotr are random**\n",
        "\n",
        "First we start by importing the needed dependencies\n",
        "* We will use torch for tensors operations\n",
        "* we will use the torch.nn for neural networks components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CkxN8Dj7A_pr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mek4ilP3UyqM"
      },
      "source": [
        "Now we start by defining our class that inherits from the nn.Module, which is the base class for all neural network modules\n",
        "* first, the constructor that serves for initializing the class.\n",
        "* then we override the forward method (represents the forward pass in a feed forward neural network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LyOH5YcaBOqU"
      },
      "outputs": [],
      "source": [
        "class Embeddings_Layer(nn.Module):\n",
        "  def __init__(self, d_model:int,vocabulary_size:int):\n",
        "    #call the constructed of the super class being the nn.Module\n",
        "    super().__init__()\n",
        "    #initialize the parameters\n",
        "    self.d_model=d_model\n",
        "    self.vocavulary_size=vocabulary_size\n",
        "    self.embeddings=nn.Embedding(vocabulary_size,d_model)\n",
        "  def forward(self,input_key:int):\n",
        "    return self.embeddings(input_key)* math.sqrt(self.d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assign values to the parameters\n",
        "* d_model reprents the size of each embedding vector of a token\n",
        "* vocabulary_size reprents the number of tokens that our vocabulary defines\n",
        "Then we create an instance of our layer"
      ],
      "metadata": {
        "id": "h4fdR4tbcsgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BwPRDLFfC5dY"
      },
      "outputs": [],
      "source": [
        "d_model=512\n",
        "vocabulary_size=100\n",
        "\n",
        "embbeding_layer=Embeddings_Layer(d_model,vocabulary_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we print the embeddings patrix where each raw is an embedding vecotr of a specific token that is represnted by an integer id"
      ],
      "metadata": {
        "id": "59GGrnkgdUTO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3w8z2A2Dt15",
        "outputId": "9f57df8e-1b9a-447a-dda1-094927debfb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.3471e+01, -2.1752e+01,  3.9809e+01, -1.1070e+01, -1.4416e+01,\n",
            "         1.8362e+01, -1.8970e+01, -5.1302e+00, -2.6344e+01, -5.7742e+00,\n",
            "        -2.1958e+01, -2.5008e+01,  2.0452e+01, -5.0389e+00,  3.1061e+01,\n",
            "        -1.0086e+01,  2.8697e+01,  9.2833e+00,  8.9686e+00,  3.2391e+00,\n",
            "         4.2731e+00, -1.7062e+01,  2.1198e+01, -1.2816e+01,  4.0075e+01,\n",
            "        -3.7147e+01, -3.4667e+01, -6.3685e+00, -2.9587e+00,  4.1154e+00,\n",
            "         3.0486e+01, -4.3666e+01, -6.4001e+00,  5.2436e+00,  1.3443e+01,\n",
            "         3.2786e+01, -1.9759e+01,  1.1936e+01, -3.8923e+00,  1.0294e+01,\n",
            "        -2.5937e+01, -8.2791e+00, -2.6847e+01, -7.0352e+00,  7.3436e+00,\n",
            "        -2.7391e+01,  1.2818e+01, -2.5097e+01,  1.6160e+01,  3.7180e+01,\n",
            "        -1.5020e+01,  2.2879e+01, -1.9451e+00, -2.1676e+01, -1.8074e+01,\n",
            "         4.6773e-01, -2.0873e+01, -1.0156e+01,  1.2170e+01,  3.4423e+01,\n",
            "         2.7031e+01,  1.6847e+01, -3.5939e+00, -3.7346e+01, -2.9247e+01,\n",
            "         4.0264e+00,  1.6102e+00,  1.1702e+01, -9.2593e+00, -2.0636e+01,\n",
            "        -6.6636e+00,  1.3063e+01, -5.1812e+01,  1.4184e+01, -1.2612e+01,\n",
            "        -1.5932e+01,  9.4236e+00, -4.8345e+00,  3.3566e+01,  2.6483e+01,\n",
            "         7.8886e+00, -1.0702e+01, -4.8901e+01,  1.8475e+01,  1.7548e+01,\n",
            "        -2.0418e+01, -1.3465e+01, -6.8438e+00, -5.0746e+01, -1.3100e+01,\n",
            "        -2.1311e+01, -8.3846e+00,  1.2667e+00,  1.5859e+01, -4.4886e+01,\n",
            "        -1.5809e+01, -2.4099e+01, -1.9639e+01,  3.5251e+01, -1.9700e+01,\n",
            "        -3.3837e-01,  3.1527e+01,  3.3584e+01,  1.1598e+01,  6.3433e+00,\n",
            "         8.0679e+00, -3.0302e+01,  3.2449e+01, -6.7967e+00,  3.1674e+01,\n",
            "        -4.0358e+01,  2.6713e+01,  4.0823e+00,  2.3841e+01, -5.9165e+01,\n",
            "         3.6731e+00,  1.8208e+01, -1.9561e+01,  5.5242e+00,  2.3675e+01,\n",
            "        -1.6210e+01,  7.9910e+00,  1.3182e+01,  2.8113e+01,  5.6253e+00,\n",
            "         1.9428e+01, -5.6206e+00,  6.5490e+00,  1.2157e+01, -1.5648e+01,\n",
            "        -2.5944e+01,  6.4117e+00,  1.6039e+01, -4.7424e+00,  1.7913e+01,\n",
            "        -4.2749e+01, -4.1943e+01,  2.0120e+01,  2.7664e+01, -2.8379e+01,\n",
            "         1.3800e+01, -8.4889e+00, -1.8014e+01,  2.9422e+01, -8.2155e+00,\n",
            "        -8.0699e+00, -1.5422e+01, -3.0347e+01, -8.3165e+00,  2.0663e+01,\n",
            "        -3.3468e+01, -1.7159e+01,  2.3748e+00,  4.2435e+01, -1.4386e+01,\n",
            "        -3.4196e-01,  8.6346e+00, -2.8151e+01,  6.5837e+00, -1.4988e+01,\n",
            "        -1.3729e+01,  1.3500e+01,  1.0524e+01, -1.4678e+01,  4.1105e+01,\n",
            "         2.2604e+01, -3.1797e+01, -3.8355e+01,  1.0104e+01,  5.7512e+00,\n",
            "        -2.8144e+01, -2.7994e+01,  2.3703e+01,  4.0637e+01, -8.5220e-01,\n",
            "         2.6215e+00, -2.1988e+00,  2.4207e+01,  2.9022e+01,  2.2748e+01,\n",
            "         6.9356e+00, -1.3834e+01,  3.4907e+01, -6.9804e+00, -9.6239e+00,\n",
            "         1.3821e+01, -7.2260e+00, -2.1877e+01, -3.7412e+01,  2.1034e+01,\n",
            "        -3.2634e+01,  1.2061e+01, -4.0942e+00,  3.8336e+00,  3.1407e+01,\n",
            "         2.2080e+01,  2.0553e+01, -1.8279e+01,  8.5241e+00, -2.0986e+01,\n",
            "        -5.6985e+01, -3.7875e+01,  1.1922e+00,  1.8287e+01, -3.1865e+01,\n",
            "        -2.3343e+01, -2.2130e+01,  6.6329e+00, -1.6809e+01,  5.1875e+00,\n",
            "         3.1287e+01,  1.4598e+01, -1.1583e+01,  1.5993e+01, -1.1056e+01,\n",
            "        -1.2564e+01, -3.0354e+00,  1.6037e+01,  7.1554e+01,  2.3002e+01,\n",
            "        -7.4680e+00, -3.5324e+01, -3.6508e+01, -2.0608e+01,  2.3037e+00,\n",
            "         5.9412e+00, -1.6731e+01,  1.1469e+01, -1.9194e+01,  6.7013e+00,\n",
            "         1.0124e+01, -7.4956e+00,  2.1619e+01, -6.5083e+00,  2.8455e+01,\n",
            "         3.8745e+00,  3.0073e+01,  7.3415e+00,  1.5124e+01, -1.1236e+01,\n",
            "        -2.5895e+01,  1.3816e+01,  1.9400e+01,  1.8777e+01, -1.7668e+00,\n",
            "         2.7585e+00, -5.1763e+00, -2.6066e+01,  6.2662e+01,  8.6055e+00,\n",
            "         9.2864e+00,  2.9490e-01, -1.3852e+01,  6.2424e+01, -2.7864e+01,\n",
            "        -1.9364e+01,  1.5265e+01, -2.7266e+01,  5.2597e+01, -1.9015e+01,\n",
            "         3.6843e+01,  2.3324e+01, -5.9126e+00, -8.3206e+00, -1.2407e+01,\n",
            "        -2.0592e+01, -4.7553e+01,  1.0608e+01,  7.7188e+00, -1.4730e+01,\n",
            "         3.7316e-01,  2.1718e+01,  5.9254e+00,  2.1891e+01, -7.1650e-01,\n",
            "        -2.4334e+01,  9.8122e+00,  1.5170e+01,  8.9816e+00,  1.4222e+01,\n",
            "        -1.2909e+01, -4.3228e+01,  4.9213e+00, -1.2167e+01, -4.0679e+01,\n",
            "        -2.0523e+01, -7.9365e+00, -1.8197e+01,  4.1883e+00,  2.1478e+01,\n",
            "        -2.5627e+01, -4.8426e+01, -6.0901e+00, -2.4117e+01,  1.6127e+01,\n",
            "        -3.8826e+01,  1.0405e+01, -1.2384e+01, -1.0116e+01, -1.9481e+01,\n",
            "        -1.9177e+01, -1.4782e+01, -1.9853e+01,  1.9974e+01,  9.0539e+00,\n",
            "         2.0884e+00,  1.5917e+01, -1.9704e+01, -4.3109e+01,  2.3664e+01,\n",
            "        -8.8460e-01,  1.3310e+01, -2.2900e+01, -2.9231e+01,  6.7135e-02,\n",
            "         1.6803e+01, -3.4300e+00,  2.3814e+01,  1.4050e+01,  1.6624e+01,\n",
            "        -1.4242e+01,  3.9308e+00, -1.1656e+00,  1.1142e+01, -1.2153e+01,\n",
            "         1.2550e+01, -9.7900e+00, -1.1134e+01, -2.9472e+01, -2.7844e+00,\n",
            "        -6.2791e+00, -6.0851e+00, -4.1866e+00,  2.9980e+00,  2.6139e+01,\n",
            "         2.9761e+01, -2.0479e+01, -1.2711e+01, -5.8447e+01, -2.4165e+01,\n",
            "         3.5593e+01, -8.4222e+00, -1.9953e+00,  1.6159e+01,  1.7836e+01,\n",
            "         1.4590e+01,  6.5647e+00,  1.4662e+01,  7.9641e+00,  1.5564e+01,\n",
            "         1.6013e+01, -1.1298e+00,  5.5167e+00, -4.9385e+01,  3.5275e+01,\n",
            "        -1.1163e+01,  6.2736e+00, -1.0916e+01, -1.8436e+01, -6.7898e+00,\n",
            "        -3.7454e+00,  1.0548e+01,  2.8835e+01,  1.7622e+01,  2.9024e+01,\n",
            "         9.6559e+00, -3.1376e+01,  1.0639e+01, -1.4364e+01,  4.8295e+01,\n",
            "        -1.6564e+01,  2.0626e+01, -2.7279e+01,  3.1283e+00,  3.2561e+01,\n",
            "        -1.2194e+01,  4.0827e+00, -3.9300e+01,  2.5170e+01,  3.5889e+01,\n",
            "        -9.5173e+00, -3.3615e+00, -1.4745e+01, -3.0605e+01,  1.3530e+00,\n",
            "         4.3995e+00, -3.7440e+01, -1.5065e+01,  1.2249e+01, -2.5385e+01,\n",
            "         1.6386e+01,  1.0367e+01,  4.3860e+01,  2.3075e+01,  7.1098e+00,\n",
            "        -7.1496e-01, -3.8430e+01,  1.0640e+01,  1.3409e+01, -2.8856e+01,\n",
            "         2.6382e+01, -8.3587e+00,  7.2490e-01, -1.3748e+01, -1.3012e+01,\n",
            "        -1.9837e+01, -2.0757e+01,  2.7978e+01, -1.8005e+00,  3.5963e+01,\n",
            "        -5.8575e+00, -2.8679e+01,  1.8751e+01, -2.7131e+00, -7.6159e+00,\n",
            "        -1.8751e-01,  4.1321e+01,  5.1826e+00,  3.3143e+01,  1.5470e+01,\n",
            "        -1.1011e+01,  4.3447e+00, -6.1249e+00, -2.6383e-01,  2.2223e+00,\n",
            "         1.4120e+01,  2.8615e+01, -2.4974e+01,  2.5517e+00, -3.3242e+01,\n",
            "        -2.9014e+01, -9.5184e+00, -2.5653e+01,  2.7342e+00, -1.1607e+01,\n",
            "        -2.9867e+00, -1.0262e+01, -3.7669e+01, -2.7807e+01, -3.5314e+01,\n",
            "        -1.2761e+01, -3.4010e+01, -1.6838e+01, -4.4555e+01, -1.6958e+01,\n",
            "        -2.9300e+01,  3.8146e-01, -7.4343e+00, -3.2356e+01, -2.8416e+00,\n",
            "         2.1389e+00, -2.4156e+01,  2.4872e+01,  1.1959e+00,  8.2189e+00,\n",
            "         7.3639e+00, -1.6891e+01, -1.1677e+01, -1.1403e+01,  9.2423e+00,\n",
            "        -2.9903e+01, -8.5350e+00, -8.2955e+00,  1.0595e+01, -1.1342e+01,\n",
            "         3.4927e+01, -1.9616e-01,  1.3343e+01,  2.8824e+00, -1.2616e+01,\n",
            "        -4.0422e+00,  1.8261e+01, -5.2294e+00, -4.4452e+00,  2.9706e+01,\n",
            "         7.3710e+00,  2.2703e+01, -2.0175e+01,  1.5488e+01,  2.7686e+01,\n",
            "         2.8088e+01,  2.6958e+00, -3.0577e+01, -5.4766e+00,  3.0319e+01,\n",
            "        -1.5961e+01, -3.9189e+01,  9.8467e+00,  5.1384e+01,  1.0621e+01,\n",
            "        -3.2283e+01,  1.1573e+00, -8.5125e+00, -2.8127e+01,  2.6540e+01,\n",
            "         2.7523e+01,  1.5408e+01, -1.6549e+01, -1.6439e+01,  4.0342e+01,\n",
            "         3.1992e+01,  1.2095e+01, -3.6912e+01,  1.5576e+01, -4.1212e+01,\n",
            "         1.7800e+01, -5.1474e+00,  4.1903e+01,  7.4989e+01, -1.5693e+01,\n",
            "         1.6390e+01,  2.3661e+01], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# the tokens_id are put inside tensors\n",
        "print(embbeding_layer(torch.tensor(0)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}